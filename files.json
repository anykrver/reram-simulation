{"src\\main.py": {"content": "\"\"\"CLI: load config, run ideal vs non-ideal crossbar, log metrics.\"\"\"\nimport argparse\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Any\n\n# Ensure project root on path when run as script\n_ROOT = Path(__file__).resolve().parent.parent\nif str(_ROOT) not in sys.path:\n    sys.path.insert(0, str(_ROOT))\n\nimport numpy as np\n\nfrom src.crossbar import IdealCrossbar, NonIdealCrossbar\nfrom src.utils.config_loader import load_config, get_crossbar_config, get_seed\nfrom src.utils.logger import get_logger\nfrom src.hardware.energy_estimator import EnergyEstimator\n\nLOG = get_logger(__name__)\n\n\ndef run_ideal(cfg: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Run ideal crossbar: I = V @ G, then compute energy.\n\n    Args:\n        cfg: Config dict with keys crossbar (rows, cols), seed.\n\n    Returns:\n        Dict with I_shape and energy_J.\n    \"\"\"\n    cb = get_crossbar_config(cfg)\n    rows, cols = cb[\"rows\"], cb[\"cols\"]\n    seed = get_seed(cfg)\n    np.random.seed(seed)\n    crossbar = IdealCrossbar(rows, cols)\n    G = np.random.rand(rows, cols).astype(np.float32) * 0.1\n    crossbar.set_conductance(G)\n    V = np.random.rand(rows).astype(np.float32) * 0.5\n    I = crossbar.run(V)\n    LOG.info(\"Ideal crossbar: I shape %s\", I.shape)\n    est = EnergyEstimator()\n    E = est.energy_crossbar(V, G)\n    LOG.info(\"Energy (J): %.6e\", E)\n    return {\"I_shape\": I.shape, \"energy_J\": float(E)}\n\n\ndef run_non_ideal(cfg: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Run non-ideal crossbar with optional noise, IR drop, variability.\n\n    Args:\n        cfg: Config with crossbar, noise, ir_drop, variability, seed.\n\n    Returns:\n        Dict with I_shape.\n    \"\"\"\n    cb = get_crossbar_config(cfg)\n    rows, cols = cb[\"rows\"], cb[\"cols\"]\n    seed = get_seed(cfg)\n    noise = cfg.get(\"noise\") or {}\n    ir_drop = cfg.get(\"ir_drop\") or {}\n    var = cfg.get(\"variability\") or {}\n    crossbar = NonIdealCrossbar(\n        rows,\n        cols,\n        noise_std=float(noise.get(\"std\", 0)) if noise.get(\"enabled\") else 0.0,\n        ir_drop_row_r=float(ir_drop.get(\"row_resistance\", 0)) if ir_drop.get(\"enabled\") else 0.0,\n        ir_drop_col_r=float(ir_drop.get(\"col_resistance\", 0)) if ir_drop.get(\"enabled\") else 0.0,\n        variability_std=float(var.get(\"conductance_std\", 0)) if var.get(\"enabled\") else 0.0,\n        seed=seed,\n    )\n    np.random.seed(seed)\n    G = np.random.rand(rows, cols).astype(np.float32) * 0.1\n    crossbar.set_conductance(G)\n    V = np.random.rand(rows).astype(np.float32) * 0.5\n    I = crossbar.run(V)\n    LOG.info(\"Non-ideal crossbar: I shape %s\", I.shape)\n    return {\"I_shape\": I.shape}\n\n\ndef main() -> int:\n    \"\"\"Entry point: parse args, load config, run selected mode. Returns exit code.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Neuro-Edge ReRAM Simulator CLI\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        default=\"configs/ideal.yaml\",\n        help=\"Path to config YAML (relative to project root or absolute)\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        choices=[\"ideal\", \"non_ideal\", \"both\"],\n        default=\"ideal\",\n        help=\"Run ideal, non_ideal, or both\",\n    )\n    args = parser.parse_args()\n    config_path = _ROOT / args.config if not Path(args.config).is_absolute() else Path(args.config)\n    if not config_path.exists():\n        LOG.error(\"Config not found: %s\", config_path)\n        return 1\n    try:\n        cfg = load_config(str(config_path))\n    except Exception as e:\n        LOG.exception(\"Failed to load config: %s\", e)\n        return 1\n    try:\n        if args.mode in (\"ideal\", \"both\"):\n            run_ideal(cfg)\n        if args.mode in (\"non_ideal\", \"both\"):\n            run_non_ideal(cfg)\n    except Exception as e:\n        LOG.exception(\"Run failed: %s\", e)\n        return 1\n    LOG.info(\"Done.\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}, "src\\__init__.py": {"content": "\"\"\"Neuro-Edge ReRAM Simulator: in-memory computing and SNN simulation.\"\"\"\n\n__version__ = \"0.1.0\"\n"}, "src\\__main__.py": {"content": "\"\"\"Allow running as python -m src.\"\"\"\nfrom src.main import main\n\nif __name__ == \"__main__\":\n    main()\n"}, "src\\crossbar\\ideal_crossbar.py": {"content": "\"\"\"Ideal ReRAM crossbar: I = V \u00d7 G (Ohm's law, matrix-vector).\"\"\"\n\nimport numpy as np\n\n\nclass IdealCrossbar:\n    \"\"\"Pure matrix multiplication: currents I = V @ G (voltages \u00d7 conductance).\"\"\"\n\n    def __init__(self, rows: int, cols: int, dtype: type = np.float32):\n        \"\"\"\n        Args:\n            rows: Number of wordlines (voltage rows).\n            cols: Number of bitlines (current columns).\n            dtype: NumPy dtype for computation.\n        \"\"\"\n        self.rows = rows\n        self.cols = cols\n        self.dtype = dtype\n        self._G = None\n\n    def set_conductance(self, G: np.ndarray) -> None:\n        \"\"\"Set conductance matrix (rows x cols). Clipped to non-negative.\"\"\"\n        G = np.asarray(G, dtype=self.dtype)\n        if G.shape != (self.rows, self.cols):\n            raise ValueError(f\"G shape {G.shape} != ({self.rows}, {self.cols})\")\n        self._G = np.maximum(G, 0.0)\n\n    def run(self, V: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute currents I = V @ G (matrix-vector or batch of vectors).\n\n        Args:\n            V: Voltages, shape (rows,) or (batch, rows).\n\n        Returns:\n            I: Currents, shape (cols,) or (batch, cols).\n        \"\"\"\n        if self._G is None:\n            raise RuntimeError(\"Conductance not set; call set_conductance first.\")\n        V = np.asarray(V, dtype=self.dtype)\n        if V.ndim == 1:\n            V = V.reshape(1, -1)\n        if V.shape[1] != self.rows:\n            raise ValueError(f\"V cols {V.shape[1]} != crossbar rows {self.rows}\")\n        I = V @ self._G\n        return I.squeeze(0) if I.shape[0] == 1 else I\n"}, "src\\crossbar\\ir_drop_model.py": {"content": "\"\"\"IR drop model: voltage drop along rows and columns (resistive interconnect).\"\"\"\n\nimport numpy as np\n\n\ndef apply_ir_drop(\n    V: np.ndarray,\n    G: np.ndarray,\n    row_resistance: float = 0.1,\n    col_resistance: float = 0.1,\n    num_steps: int = 5,\n) -> np.ndarray:\n    \"\"\"\n    Apply iterative IR drop to get effective voltages at each cell.\n\n    Simplified model: voltage at (i,j) reduced by current flowing through\n    row/column resistance. Returns effective V matrix same shape as implied by V, G.\n\n    Args:\n        V: Applied voltages (rows,) or (rows,).\n        G: Conductance matrix (rows, cols).\n        row_resistance: Resistance per row segment (Ohms).\n        col_resistance: Resistance per column segment (Ohms).\n        num_steps: Iterations for convergence.\n\n    Returns:\n        V_eff: Effective voltages (rows, cols) for energy/current computation.\n    \"\"\"\n    V = np.asarray(V, dtype=np.float64)\n    G = np.asarray(G, dtype=np.float64)\n    if V.ndim == 1:\n        V = V[:, np.newaxis]\n    rows, cols = G.shape\n    V_eff = np.broadcast_to(V, (rows, cols)).copy()\n    for _ in range(num_steps):\n        I_col = (V_eff * G).sum(axis=0)\n        I_row = (V_eff * G).sum(axis=1)\n        drop_col = np.cumsum(np.insert(I_col * col_resistance, 0, 0))[:-1]\n        drop_row = np.cumsum(np.insert(I_row * row_resistance, 0, 0))[:-1]\n        V_eff = np.maximum(\n            V - drop_row[:, np.newaxis] - drop_col[np.newaxis, :],\n            0.0,\n        )\n    return V_eff.astype(G.dtype)\n"}, "src\\crossbar\\non_ideal_crossbar.py": {"content": "\"\"\"Non-ideal ReRAM crossbar: ideal + IR drop, quantization, variability.\"\"\"\n\nfrom typing import Optional\n\nimport numpy as np\n\nfrom .ideal_crossbar import IdealCrossbar\nfrom .ir_drop_model import apply_ir_drop\nfrom .quantization import quantize_weights, quantize_activations\nfrom .variability import add_conductance_variability\n\n\nclass NonIdealCrossbar:\n    \"\"\"\n    Crossbar with optional noise, IR drop, variability, quantization.\n    Wraps ideal I = V @ G and applies non-ideal effects.\n    \"\"\"\n\n    def __init__(\n        self,\n        rows: int,\n        cols: int,\n        noise_std: float = 0.0,\n        ir_drop_row_r: float = 0.0,\n        ir_drop_col_r: float = 0.0,\n        variability_std: float = 0.0,\n        quantize_bits: Optional[int] = None,\n        seed: Optional[int] = None,\n        dtype: type = np.float32,\n    ):\n        \"\"\"\n        Args:\n            rows: Wordlines.\n            cols: Bitlines.\n            noise_std: Additive conductance noise (0 = off).\n            ir_drop_row_r: Row resistance for IR drop (0 = off).\n            ir_drop_col_r: Column resistance for IR drop (0 = off).\n            variability_std: Conductance variability (0 = off).\n            quantize_bits: Weight/activation bits (None = off).\n            seed: RNG seed.\n            dtype: Compute dtype.\n        \"\"\"\n        self.rows = rows\n        self.cols = cols\n        self.noise_std = noise_std\n        self.ir_drop_row_r = ir_drop_row_r\n        self.ir_drop_col_r = ir_drop_col_r\n        self.variability_std = variability_std\n        self.quantize_bits = quantize_bits\n        self.seed = seed\n        self.dtype = dtype\n        self._ideal = IdealCrossbar(rows, cols, dtype)\n        self._G_nominal = None\n\n    def set_conductance(self, G: np.ndarray) -> None:\n        \"\"\"Set nominal conductance; apply variability and optional quantization.\"\"\"\n        G = np.asarray(G, dtype=self.dtype)\n        if G.shape != (self.rows, self.cols):\n            raise ValueError(f\"G shape {G.shape} != ({self.rows}, {self.cols})\")\n        self._G_nominal = np.maximum(G, 0.0).copy()\n        G_eff = self._G_nominal.copy()\n        if self.variability_std > 0:\n            G_eff = add_conductance_variability(\n                G_eff, std=self.variability_std, seed=self.seed\n            )\n        if self.quantize_bits is not None:\n            G_eff = quantize_weights(G_eff, bits=self.quantize_bits)\n        self._ideal.set_conductance(G_eff)\n\n    def run(self, V: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Run with optional IR drop and activation quantization.\n        Currents computed from effective V and stored G.\n        \"\"\"\n        if self._G_nominal is None:\n            raise RuntimeError(\"Conductance not set.\")\n        V = np.asarray(V, dtype=self.dtype)\n        G = self._ideal._G\n        if self.quantize_bits is not None:\n            V = quantize_activations(V, bits=self.quantize_bits)\n        if self.ir_drop_row_r > 0 or self.ir_drop_col_r > 0:\n            V_1d = V.ravel()\n            V_2d = V_1d.reshape(-1, 1) if V.ndim == 1 else V\n            V_eff = apply_ir_drop(\n                V_2d,\n                G,\n                row_resistance=self.ir_drop_row_r,\n                col_resistance=self.ir_drop_col_r,\n            )\n            I = (V_eff * G).sum(axis=0)\n            out = I.astype(self.dtype)\n            return out if V.ndim == 1 else out.reshape(1, -1)\n        if self.noise_std > 0:\n            rng = np.random.default_rng(self.seed)\n            G_noisy = np.maximum(G + rng.normal(0, self.noise_std, G.shape), 0.0)\n            self._ideal.set_conductance(G_noisy)\n        I = self._ideal.run(V)\n        return I\n"}, "src\\crossbar\\quantization.py": {"content": "\"\"\"Weight and activation quantization (e.g. int8 / fixed-point).\"\"\"\n\nfrom typing import Optional\n\nimport numpy as np\n\n\ndef quantize_weights(\n    W: np.ndarray,\n    bits: int = 8,\n    symmetric: bool = True,\n) -> np.ndarray:\n    \"\"\"\n    Quantize weights to signed fixed-point.\n\n    Args:\n        W: Full-precision weights.\n        bits: Number of bits (excluding sign if symmetric).\n        symmetric: If True, range is [-2^(bits-1), 2^(bits-1)-1].\n\n    Returns:\n        Quantized float representation (dequantized for use in crossbar).\n    \"\"\"\n    W = np.asarray(W)\n    n_levels = 2 ** bits\n    if symmetric:\n        scale = np.abs(W).max()\n        if scale == 0:\n            return W\n        scale = scale / (n_levels // 2 - 1)\n        q = np.round(W / scale).astype(np.int32)\n        q = np.clip(q, -n_levels // 2, n_levels // 2 - 1)\n        return (q * scale).astype(W.dtype)\n    else:\n        w_min, w_max = W.min(), W.max()\n        scale = (w_max - w_min) / (n_levels - 1) if w_max > w_min else 1.0\n        q = np.round((W - w_min) / scale).astype(np.int32)\n        q = np.clip(q, 0, n_levels - 1)\n        return (q * scale + w_min).astype(W.dtype)\n\n\ndef quantize_activations(x: np.ndarray, bits: int = 8) -> np.ndarray:\n    \"\"\"\n    Quantize activations (e.g. voltages) to unsigned fixed-point [0, 2^bits - 1].\n\n    Args:\n        x: Activations (non-negative for ReRAM).\n        bits: Number of bits.\n\n    Returns:\n        Dequantized float in same range.\n    \"\"\"\n    x = np.asarray(x)\n    x = np.maximum(x, 0.0)\n    n_levels = 2 ** bits\n    scale = x.max() / (n_levels - 1) if x.max() > 0 else 1.0\n    q = np.round(x / scale).astype(np.int32)\n    q = np.clip(q, 0, n_levels - 1)\n    return (q * scale).astype(x.dtype)\n"}, "src\\crossbar\\variability.py": {"content": "\"\"\"Conductance/weight variability (e.g. Gaussian, cycle-to-cycle).\"\"\"\n\nfrom typing import Optional\n\nimport numpy as np\n\n\ndef add_conductance_variability(\n    G: np.ndarray,\n    std: float = 0.02,\n    seed: Optional[int] = None,\n) -> np.ndarray:\n    \"\"\"\n    Add Gaussian noise to conductance (cycle-to-cycle or device-to-device).\n\n    Args:\n        G: Nominal conductance matrix.\n        std: Standard deviation (fraction or absolute, applied as additive).\n        seed: Random seed for reproducibility.\n\n    Returns:\n        G_noisy: G + noise, clipped to non-negative.\n    \"\"\"\n    G = np.asarray(G, dtype=np.float64)\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(0, std, G.shape)\n    G_noisy = np.maximum(G + noise, 0.0)\n    return G_noisy.astype(G.dtype)\n\n\ndef add_variability_percent(\n    G: np.ndarray,\n    cv: float = 0.05,\n    seed: Optional[int] = None,\n) -> np.ndarray:\n    \"\"\"\n    Multiplicative variability: G * (1 + N(0, cv^2)).\n\n    Args:\n        G: Nominal conductance.\n        cv: Coefficient of variation (std/mean).\n        seed: Random seed.\n\n    Returns:\n        G_noisy: Non-negative conductance.\n    \"\"\"\n    G = np.asarray(G, dtype=np.float64)\n    rng = np.random.default_rng(seed)\n    factor = 1.0 + rng.normal(0, cv, G.shape)\n    G_noisy = np.maximum(G * factor, 0.0)\n    return G_noisy.astype(G.dtype)\n"}, "src\\crossbar\\__init__.py": {"content": "\"\"\"ReRAM crossbar simulation: ideal and non-ideal models.\"\"\"\n\nfrom .ideal_crossbar import IdealCrossbar\nfrom .non_ideal_crossbar import NonIdealCrossbar\n\n__all__ = [\"IdealCrossbar\", \"NonIdealCrossbar\"]\n"}, "src\\hardware\\accelerator_model.py": {"content": "\"\"\"Abstract accelerator: tiles, crossbars, memory.\"\"\"\n\n\nclass AcceleratorModel:\n    def __init__(self, num_tiles: int = 1, rows_per_crossbar: int = 64, cols_per_crossbar: int = 64,\n                 memory_energy_per_word_pJ: float = 1.0):\n        self.num_tiles = num_tiles\n        self.rows_per_crossbar = rows_per_crossbar\n        self.cols_per_crossbar = cols_per_crossbar\n        self.memory_energy_per_word_pJ = memory_energy_per_word_pJ\n\n    def total_crossbar_elements(self) -> int:\n        return self.num_tiles * self.rows_per_crossbar * self.cols_per_crossbar\n\n    def memory_energy(self, words_read: int = 0, words_written: int = 0) -> float:\n        return (words_read + words_written) * self.memory_energy_per_word_pJ\n"}, "src\\hardware\\controller_logic.py": {"content": "\"\"\"Crossbar controller: load weights, apply V, read I.\"\"\"\n\nfrom typing import Callable, Optional\n\nimport numpy as np\n\n\nclass CrossbarController:\n    \"\"\"FSM: load weights, run V, sample I.\"\"\"\n\n    def __init__(\n        self,\n        set_conductance: Callable[[np.ndarray], None],\n        run: Callable[[np.ndarray], np.ndarray],\n        rows: int,\n        cols: int,\n    ):\n        self.set_conductance = set_conductance\n        self.run = run\n        self.rows = rows\n        self.cols = cols\n        self._G: Optional[np.ndarray] = None\n\n    def load_weights(self, G: np.ndarray) -> None:\n        G = np.asarray(G)\n        if G.shape != (self.rows, self.cols):\n            raise ValueError(\"G shape mismatch\")\n        self._G = G.copy()\n        self.set_conductance(G)\n\n    def execute(self, V: np.ndarray) -> np.ndarray:\n        if self._G is None:\n            raise RuntimeError(\"Weights not loaded\")\n        return self.run(V)\n"}, "src\\hardware\\energy_estimator.py": {"content": "\"\"\"Energy: E = V^2 * G * t.\"\"\"\n\nfrom typing import Optional\nimport numpy as np\n\n\nclass EnergyEstimator:\n    def __init__(self, voltage: float = 1.0, timestep_s: float = 1e-6):\n        self.voltage = voltage\n        self.timestep_s = timestep_s\n\n    def energy_crossbar(self, V: np.ndarray, G: np.ndarray, t_s: Optional[float] = None) -> float:\n        V = np.asarray(V, dtype=np.float64)\n        G = np.asarray(G, dtype=np.float64)\n        t = t_s if t_s is not None else self.timestep_s\n        if V.ndim == 1:\n            V = V[:, np.newaxis]\n        V_eff = np.broadcast_to(V, G.shape)\n        return float(np.sum(V_eff ** 2 * G) * t)\n\n    def latency_cycles(self, rows: int, cols: int, cycles_per_op: int = 1) -> int:\n        return rows * cols * cycles_per_op\n"}, "src\\hardware\\__init__.py": {"content": "from .energy_estimator import EnergyEstimator\nfrom .accelerator_model import AcceleratorModel\nfrom .controller_logic import CrossbarController\n__all__ = [\"EnergyEstimator\", \"AcceleratorModel\", \"CrossbarController\"]\n"}, "src\\snn\\network.py": {"content": "\"\"\"SNN network: layers, timesteps, uses crossbar for weight multiply.\"\"\"\n\nfrom typing import Optional, Callable\n\nimport numpy as np\n\nfrom .spike_encoder import PoissonEncoder\nfrom .neuron import LIFNeuron\n\n\nclass SNNNetwork:\n    \"\"\"Single-layer SNN that uses a crossbar for dense weight multiplication.\"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        crossbar_run: Callable[[np.ndarray], np.ndarray],\n        encoder: Optional[PoissonEncoder] = None,\n        neuron: Optional[LIFNeuron] = None,\n        timesteps: int = 100,\n    ):\n        \"\"\"\n        Args:\n            input_size: Number of input units.\n            output_size: Number of output (LIF) neurons.\n            crossbar_run: Function V -> I (voltages to currents); expects V (timesteps, input_size) or (input_size,).\n            encoder: Spike encoder; default PoissonEncoder().\n            neuron: LIF layer; default LIFNeuron().\n            timesteps: Simulation timesteps.\n        \"\"\"\n        self.input_size = input_size\n        self.output_size = output_size\n        self.crossbar_run = crossbar_run\n        self.encoder = encoder or PoissonEncoder()\n        self.neuron = neuron or LIFNeuron()\n        self.timesteps = timesteps\n\n    def forward(\n        self,\n        x: np.ndarray,\n        seed: Optional[int] = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Run SNN: encode x -> spikes, crossbar(V) -> I, LIF -> output spikes.\n\n        Args:\n            x: Input (input_size,) normalized to [0,1].\n            seed: Encoder RNG seed.\n\n        Returns:\n            output_spikes: (timesteps, output_size).\n            total_spikes: (output_size,) per-neuron spike counts.\n        \"\"\"\n        x = np.asarray(x).ravel()\n        if x.size != self.input_size:\n            raise ValueError(f\"x size {x.size} != input_size {self.input_size}\")\n        spikes_in = self.encoder.encode(x, self.timesteps, seed=seed)\n        output_spikes = np.zeros((self.timesteps, self.output_size), dtype=np.float32)\n        membrane = None\n        for t in range(self.timesteps):\n            V = spikes_in[t]\n            I = self.crossbar_run(V)\n            I = np.asarray(I).ravel()\n            if I.size != self.output_size:\n                I = np.resize(I, self.output_size)\n            spk, membrane = self.neuron.step(I, membrane)\n            output_spikes[t] = spk\n        total_spikes = output_spikes.sum(axis=0)\n        return output_spikes, total_spikes\n"}, "src\\snn\\neuron.py": {"content": "\"\"\"LIF / threshold neurons: membrane, fire, reset.\"\"\"\n\nfrom typing import Optional\n\nimport numpy as np\n\n\nclass LIFNeuron:\n    \"\"\"Leaky integrate-and-fire neuron.\"\"\"\n\n    def __init__(\n        self,\n        threshold: float = 1.0,\n        leak: float = 0.99,\n        reset: float = 0.0,\n        dtype: type = np.float32,\n    ):\n        \"\"\"\n        Args:\n            threshold: Spike threshold (V).\n            leak: Membrane leak factor per timestep (V *= leak).\n            reset: Membrane potential after spike.\n            dtype: Compute dtype.\n        \"\"\"\n        self.threshold = threshold\n        self.leak = leak\n        self.reset = reset\n        self.dtype = dtype\n\n    def step(\n        self,\n        current: np.ndarray,\n        membrane: Optional[np.ndarray] = None,\n    ):\n        \"\"\"\n        One timestep: integrate current, leak, threshold, reset.\n\n        Args:\n            current: Input current (e.g. from crossbar), shape (n_neurons,).\n            membrane: Previous membrane state; if None, zeros.\n\n        Returns:\n            spikes: Binary spikes shape (n_neurons,).\n            membrane_new: New membrane state (n_neurons,).\n        \"\"\"\n        current = np.asarray(current, dtype=self.dtype)\n        n = current.size\n        if membrane is None:\n            membrane = np.zeros(n, dtype=self.dtype)\n        else:\n            membrane = np.asarray(membrane, dtype=self.dtype)\n        membrane = membrane * self.leak + current\n        spikes = (membrane >= self.threshold).astype(self.dtype)\n        membrane = np.where(spikes > 0, self.reset, membrane)\n        return spikes, membrane\n"}, "src\\snn\\spike_encoder.py": {"content": "\"\"\"Spike encoding: rate (Poisson) or temporal.\"\"\"\n\nfrom typing import Optional\n\nimport numpy as np\n\n\nclass PoissonEncoder:\n    \"\"\"Encode continuous values to spike trains via Poisson process.\"\"\"\n\n    def __init__(\n        self,\n        max_rate: float = 100.0,\n        timestep_ms: float = 1.0,\n        seed: Optional[int] = None,\n    ):\n        \"\"\"\n        Args:\n            max_rate: Maximum firing rate (Hz).\n            timestep_ms: Timestep duration in ms.\n            seed: Random seed for reproducibility.\n        \"\"\"\n        self.max_rate = max_rate\n        self.timestep_ms = timestep_ms\n        self.seed = seed\n        self._rng = np.random.default_rng(seed)\n\n    def encode(\n        self,\n        x: np.ndarray,\n        num_timesteps: int,\n        seed: Optional[int] = None,\n    ) -> np.ndarray:\n        \"\"\"\n        Encode x (values in [0, 1]) to spikes (num_timesteps, ...).\n\n        Args:\n            x: Input values, shape (...,). Normalized to [0,1] for rate.\n            num_timesteps: Number of time steps.\n            seed: Override seed for this call.\n\n        Returns:\n            spikes: Binary array shape (num_timesteps,) + x.shape.\n        \"\"\"\n        x = np.asarray(x, dtype=np.float64)\n        x = np.clip(x, 0.0, 1.0)\n        rng = np.random.default_rng(seed if seed is not None else self.seed)\n        rate = x * self.max_rate * (self.timestep_ms / 1000.0)\n        shape = (num_timesteps,) + x.shape\n        spikes = rng.random(shape) < np.broadcast_to(rate, shape)\n        return spikes.astype(np.float32)\n"}, "src\\snn\\trainer.py": {"content": "\"\"\"Rate-coded surrogate-gradient trainer for the crossbar SNN.\n\nTraining intuition\n------------------\nIn the rate approximation the expected total output spikes for sample x is:\n\n    total_spikes \u2248 mean_rate_in(x) @ G      (linear in G)\n\nwhere mean_rate_in(x) = x * max_rate * dt is the expected input spike rate.\n\nWe minimise cross-entropy loss on softmax(total_spikes / T) with\nstandard SGD and project G to the non-negative orthant after each step\n(conductance cannot be negative).\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Optional, Tuple\n\nimport numpy as np\n\nfrom .spike_encoder import PoissonEncoder\n\n\ndef _softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically stable row-wise softmax.\"\"\"\n    z = z - z.max(axis=-1, keepdims=True)\n    e = np.exp(z)\n    return e / e.sum(axis=-1, keepdims=True)\n\n\ndef _one_hot(y: np.ndarray, n_classes: int) -> np.ndarray:\n    \"\"\"Convert integer labels (N,) to one-hot (N, n_classes).\"\"\"\n    oh = np.zeros((y.size, n_classes), dtype=np.float32)\n    oh[np.arange(y.size), y] = 1.0\n    return oh\n\n\nclass SNNTrainer:\n    \"\"\"\n    Trains crossbar conductances G via a rate-coded surrogate gradient.\n\n    The forward pass computes expected spike rates analytically\n    (mean_rate_in = x * max_rate * dt) and uses I = mean_rate_in @ G as\n    the logit.  The gradient flows exactly through this linear map.\n\n    Parameters\n    ----------\n    n_in:       Number of input neurons (e.g. 784 for MNIST).\n    n_out:      Number of output classes (e.g. 10).\n    lr:         Learning rate for SGD.\n    max_rate:   PoissonEncoder max firing rate (Hz). Used only to scale\n                the analytic mean-rate; must match the encoder used at\n                inference time.\n    timestep_ms: Encoder timestep (ms).\n    timesteps:  Number of SNN timesteps (used to normalise logits).\n    G_max:      Upper bound for conductance clipping (S).\n    seed:       Optional RNG seed for weight initialisation.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_in: int,\n        n_out: int,\n        lr: float = 0.01,\n        max_rate: float = 100.0,\n        timestep_ms: float = 1.0,\n        timesteps: int = 50,\n        G_max: float = 1.0,\n        seed: Optional[int] = None,\n    ):\n        self.n_in = n_in\n        self.n_out = n_out\n        self.lr = lr\n        self.max_rate = max_rate\n        self.timestep_ms = timestep_ms\n        self.timesteps = timesteps\n        self.G_max = G_max\n        self._dt = max_rate * timestep_ms / 1000.0  # firing prob per timestep per unit input\n\n        rng = np.random.default_rng(seed)\n        # Small positive initialisation (conductances \u2265 0)\n        self.G = rng.uniform(0.0, 0.01, (n_in, n_out)).astype(np.float32)\n\n    # ------------------------------------------------------------------\n    # Public helpers\n    # ------------------------------------------------------------------\n\n    def get_weights(self) -> np.ndarray:\n        \"\"\"Return current conductance matrix G (n_in, n_out).\"\"\"\n        return self.G.copy()\n\n    def set_weights(self, G: np.ndarray) -> None:\n        \"\"\"Set conductance matrix; projected to [0, G_max].\"\"\"\n        G = np.asarray(G, dtype=np.float32)\n        if G.shape != (self.n_in, self.n_out):\n            raise ValueError(f\"Expected G shape ({self.n_in}, {self.n_out}), got {G.shape}\")\n        self.G = np.clip(G, 0.0, self.G_max)\n\n    # ------------------------------------------------------------------\n    # Forward\n    # ------------------------------------------------------------------\n\n    def mean_rate_in(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute analytic mean input spike rate for a batch.\n\n        Parameters\n        ----------\n        X : (N, n_in) float32 in [0, 1].\n\n        Returns\n        -------\n        rates : (N, n_in) float32.\n        \"\"\"\n        X = np.asarray(X, dtype=np.float32)\n        return np.clip(X, 0.0, 1.0) * self._dt * self.timesteps\n\n    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Rate-coded forward pass.\n\n        Parameters\n        ----------\n        X : (N, n_in) batch of inputs in [0, 1].\n\n        Returns\n        -------\n        logits       : (N, n_out) \u2014 I = mean_rate_in @ G.\n        mean_rates   : (N, n_in) \u2014 cached for gradient.\n        \"\"\"\n        rates = self.mean_rate_in(X)\n        logits = rates @ self.G          # (N, n_out)\n        return logits, rates\n\n    # ------------------------------------------------------------------\n    # Loss and gradient\n    # ------------------------------------------------------------------\n\n    def loss_and_grad(\n        self,\n        logits: np.ndarray,\n        mean_rates: np.ndarray,\n        y: np.ndarray,\n    ) -> Tuple[float, np.ndarray]:\n        \"\"\"\n        Cross-entropy loss and gradient dL/dG.\n\n        Parameters\n        ----------\n        logits      : (N, n_out).\n        mean_rates  : (N, n_in).\n        y           : (N,) integer labels.\n\n        Returns\n        -------\n        loss : scalar float.\n        dG   : (n_in, n_out) gradient.\n        \"\"\"\n        N = logits.shape[0]\n        probs = _softmax(logits)                     # (N, n_out)\n        oh = _one_hot(y, self.n_out)                 # (N, n_out)\n        # Cross-entropy\n        eps = 1e-9\n        loss = -float(np.mean(np.sum(oh * np.log(probs + eps), axis=1)))\n        delta = (probs - oh) / N                     # (N, n_out)\n        dG = mean_rates.T @ delta                    # (n_in, n_out)\n        return loss, dG.astype(np.float32)\n\n    # ------------------------------------------------------------------\n    # Optimiser step\n    # ------------------------------------------------------------------\n\n    def step(self, dG: np.ndarray) -> None:\n        \"\"\"\n        SGD update + non-negativity + G_max projection.\n\n        Parameters\n        ----------\n        dG : (n_in, n_out) gradient from loss_and_grad.\n        \"\"\"\n        self.G -= self.lr * dG\n        np.clip(self.G, 0.0, self.G_max, out=self.G)\n\n    # ------------------------------------------------------------------\n    # Convenience: one full training step\n    # ------------------------------------------------------------------\n\n    def train_batch(\n        self, X: np.ndarray, y: np.ndarray\n    ) -> float:\n        \"\"\"\n        Forward + loss + backward + weight update for one mini-batch.\n\n        Parameters\n        ----------\n        X : (N, n_in) inputs in [0, 1].\n        y : (N,) integer labels.\n\n        Returns\n        -------\n        loss : scalar cross-entropy loss.\n        \"\"\"\n        logits, rates = self.forward(X)\n        loss, dG = self.loss_and_grad(logits, rates, y)\n        self.step(dG)\n        return loss\n"}, "src\\snn\\__init__.py": {"content": "\"\"\"Spiking neural network: encoding, neurons, network, training.\"\"\"\n\nfrom .spike_encoder import PoissonEncoder\nfrom .neuron import LIFNeuron\nfrom .network import SNNNetwork\nfrom .trainer import SNNTrainer\n\n__all__ = [\"PoissonEncoder\", \"LIFNeuron\", \"SNNNetwork\", \"SNNTrainer\"]\n"}, "src\\utils\\config_loader.py": {"content": "\"\"\"Load and validate YAML configs for experiments and CLI.\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport yaml\n\n\ndef load_config(path: str) -> Dict[str, Any]:\n    \"\"\"\n    Load a YAML config file.\n\n    Args:\n        path: Path to .yaml file (relative to cwd or absolute).\n\n    Returns:\n        Nested dict of config. Empty dict if file is empty.\n\n    Raises:\n        FileNotFoundError: If path does not exist.\n        yaml.YAMLError: If YAML is invalid.\n    \"\"\"\n    p = Path(path).resolve()\n    if not p.exists():\n        raise FileNotFoundError(f\"Config not found: {p}\")\n    if not p.is_file():\n        raise ValueError(f\"Not a file: {p}\")\n    with open(p, \"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f)\n    return data if data is not None else {}\n\n\ndef get_crossbar_config(cfg: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Extract crossbar config with defaults. Safe for missing keys.\"\"\"\n    cb = cfg.get(\"crossbar\") or {}\n    return {\n        \"rows\": int(cb.get(\"rows\", 64)),\n        \"cols\": int(cb.get(\"cols\", 64)),\n        \"precision\": cb.get(\"precision\", \"float32\"),\n    }\n\n\ndef get_seed(cfg: Dict[str, Any]) -> int:\n    \"\"\"Extract seed with default 42.\"\"\"\n    return int(cfg.get(\"seed\", 42))\n"}, "src\\utils\\logger.py": {"content": "\"\"\"Centralized logging (logging + optional file handler).\"\"\"\n\nimport logging\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\n\n\ndef get_logger(\n    name: str = \"neuro_edge_reram\",\n    level: int = logging.INFO,\n    log_file: Optional[str] = None,\n) -> logging.Logger:\n    \"\"\"\n    Return a configured logger.\n\n    Args:\n        name: Logger name.\n        level: Logging level (default INFO).\n        log_file: If set, also log to this file.\n\n    Returns:\n        Configured Logger.\n    \"\"\"\n    logger = logging.getLogger(name)\n    if logger.handlers:\n        return logger\n    logger.setLevel(level)\n    fmt = logging.Formatter(\n        \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n    )\n    sh = logging.StreamHandler(sys.stdout)\n    sh.setFormatter(fmt)\n    logger.addHandler(sh)\n    if log_file:\n        Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n        fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n        fh.setFormatter(fmt)\n        logger.addHandler(fh)\n    return logger\n"}, "src\\utils\\metrics.py": {"content": "\"\"\"Accuracy, energy, latency aggregation.\"\"\"\n\nfrom typing import List, Optional\n\nimport numpy as np\n\n\ndef compute_accuracy(\n    predictions: np.ndarray,\n    targets: np.ndarray,\n    top_k: int = 1,\n) -> float:\n    \"\"\"\n    Classification accuracy (top-k).\n\n    Args:\n        predictions: Logits or class scores (N, C).\n        targets: Integer labels (N,).\n        top_k: Count prediction correct if target in top-k.\n\n    Returns:\n        Accuracy in [0, 1].\n    \"\"\"\n    predictions = np.asarray(predictions)\n    targets = np.asarray(targets, dtype=np.intp)\n    if predictions.ndim == 1:\n        predictions = predictions.reshape(1, -1)\n    if targets.ndim != 1:\n        targets = targets.ravel()\n    N = targets.size\n    if N == 0:\n        return 0.0\n    top = np.argsort(-predictions, axis=1)[:, :top_k]\n    correct = np.any(top == targets[:, np.newaxis], axis=1)\n    return float(np.mean(correct))\n\n\ndef aggregate_energy(\n    energy_list: List[float],\n    timesteps: Optional[int] = None,\n) -> dict:\n    \"\"\"\n    Aggregate energy over runs.\n\n    Args:\n        energy_list: Per-step or per-run energy (J).\n        timesteps: If set, reshape as (runs, timesteps) and sum per run.\n\n    Returns:\n        Dict with total, mean, std.\n    \"\"\"\n    arr = np.asarray(energy_list)\n    total = float(np.sum(arr))\n    return {\n        \"total_J\": total,\n        \"mean_J\": float(np.mean(arr)),\n        \"std_J\": float(np.std(arr)) if arr.size > 1 else 0.0,\n        \"count\": int(arr.size),\n    }\n"}, "src\\utils\\mnist_loader.py": {"content": "\"\"\"Load MNIST from kagglehub path (IDX or CSV). Used by experiments and dashboard.\"\"\"\nfrom pathlib import Path\nimport numpy as np\n\n\ndef get_mnist_path():\n    \"\"\"Download MNIST via kagglehub; return path to dataset files.\"\"\"\n    import kagglehub\n    return kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n\n\ndef _read_idx_images(path, max_items=None):\n    \"\"\"Read MNIST IDX images (idx3-ubyte). Returns (N, H*W) float32 [0,1].\"\"\"\n    with open(path, \"rb\") as f:\n        f.read(4)  # magic\n        n = int.from_bytes(f.read(4), \"big\")\n        rows = int.from_bytes(f.read(4), \"big\")\n        cols = int.from_bytes(f.read(4), \"big\")\n        if max_items is not None:\n            n = min(n, max_items)\n        data = np.frombuffer(f.read(n * rows * cols), dtype=np.uint8)\n    data = data.reshape(n, rows * cols).astype(np.float32) / 255.0\n    return np.clip(data, 0.0, 1.0)\n\n\ndef _read_idx_labels(path, max_items=None):\n    \"\"\"Read MNIST IDX labels (idx1-ubyte). Returns (N,) int32.\"\"\"\n    with open(path, \"rb\") as f:\n        f.read(4)  # magic\n        n = int.from_bytes(f.read(4), \"big\")\n        if max_items is not None:\n            n = min(n, max_items)\n        data = np.frombuffer(f.read(n), dtype=np.uint8)\n    return data.astype(np.int32)\n\n\ndef load_mnist_from_path(data_path, max_train=1000, max_test=200):\n    \"\"\"Load MNIST from kagglehub path. Supports IDX or CSV. Returns X_train, y_train, X_test, y_test, n_pixels.\"\"\"\n    data_path = Path(data_path)\n    train_img = data_path / \"train-images.idx3-ubyte\"\n    train_lbl = data_path / \"train-labels.idx1-ubyte\"\n    t10k_img = data_path / \"t10k-images.idx3-ubyte\"\n    t10k_lbl = data_path / \"t10k-labels.idx1-ubyte\"\n    if train_img.exists() and train_lbl.exists():\n        X_train = _read_idx_images(str(train_img), max_items=max_train)\n        y_train = _read_idx_labels(str(train_lbl), max_items=max_train)\n        if t10k_img.exists() and t10k_lbl.exists():\n            X_test = _read_idx_images(str(t10k_img), max_items=max_test)\n            y_test = _read_idx_labels(str(t10k_lbl), max_items=max_test)\n        else:\n            X_test = X_train[-max_test:]\n            y_test = y_train[-max_test:]\n        return X_train, y_train, X_test, y_test, X_train.shape[1]\n    import pandas as pd\n    files = list(data_path.rglob(\"*.csv\"))\n    if not files:\n        raise FileNotFoundError(f\"No IDX or CSV files in {data_path}\")\n    train_files = [f for f in files if \"train\" in f.name.lower()]\n    test_files = [f for f in files if \"test\" in f.name.lower()]\n    train_csv = train_files[0] if train_files else files[0]\n    test_csv = test_files[0] if test_files else (files[1] if len(files) > 1 else files[0])\n    train_df = pd.read_csv(train_csv, nrows=max_train)\n    test_df = pd.read_csv(test_csv, nrows=max_test) if train_csv != test_csv else train_df.tail(max_test)\n    label_col = \"label\" if \"label\" in train_df.columns else train_df.columns[0]\n    pixel_cols = [c for c in train_df.columns if c != label_col]\n    if not pixel_cols:\n        pixel_cols = list(train_df.columns[1:])\n    n_pixels = len(pixel_cols)\n    X_train = np.clip(train_df[pixel_cols].values.astype(np.float32) / 255.0, 0, 1)\n    y_train = train_df[label_col].values.astype(np.int32)\n    X_test = np.clip(test_df[pixel_cols].values.astype(np.float32) / 255.0, 0, 1)\n    y_test = test_df[label_col].values.astype(np.int32)\n    return X_train, y_train, X_test, y_test, n_pixels\n"}, "src\\utils\\visualization.py": {"content": "\"\"\"Heatmaps, firing raster, power curves (for dashboard/notebooks).\"\"\"\n\nfrom typing import Optional\n\nimport numpy as np\n\ntry:\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    _HAS_MPL = True\nexcept ImportError:\n    _HAS_MPL = False\n\n\ndef plot_heatmap(\n    data: np.ndarray,\n    title: str = \"Heatmap\",\n    xlabel: str = \"Columns\",\n    ylabel: str = \"Rows\",\n    figsize: tuple = (6, 5),\n):\n    \"\"\"\n    Plot 2D heatmap (e.g. conductance matrix).\n\n    Args:\n        data: 2D array.\n        title: Plot title.\n        xlabel: X axis label.\n        ylabel: Y axis label.\n        figsize: Figure size.\n\n    Returns:\n        matplotlib Figure or None if matplotlib not available.\n    \"\"\"\n    if not _HAS_MPL:\n        return None\n    data = np.asarray(data)\n    fig, ax = plt.subplots(figsize=figsize)\n    im = ax.imshow(data, aspect=\"auto\", cmap=\"viridis\")\n    plt.colorbar(im, ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    plt.tight_layout()\n    return fig\n\n\ndef plot_firing_raster(\n    spikes: np.ndarray,\n    title: str = \"Firing raster\",\n    xlabel: str = \"Time\",\n    ylabel: str = \"Neuron\",\n    figsize: tuple = (8, 4),\n):\n    \"\"\"\n    Raster plot: time vs neuron index, dot where spike.\n\n    Args:\n        spikes: (timesteps, neurons) binary.\n        title: Plot title.\n        xlabel: X axis label.\n        ylabel: Y axis label.\n        figsize: Figure size.\n\n    Returns:\n        matplotlib Figure or None.\n    \"\"\"\n    if not _HAS_MPL:\n        return None\n    spikes = np.asarray(spikes)\n    if spikes.ndim != 2:\n        spikes = spikes.reshape(-1, spikes.size)\n    fig, ax = plt.subplots(figsize=figsize)\n    t_axis = np.arange(spikes.shape[0])\n    n_axis = np.arange(spikes.shape[1])\n    t_grid, n_grid = np.meshgrid(t_axis, n_axis, indexing=\"ij\")\n    mask = spikes > 0\n    ax.scatter(t_grid[mask], n_grid[mask], c=\"k\", s=1, alpha=0.8)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    plt.tight_layout()\n    return fig\n\n\ndef plot_power_curve(\n    time_ms: np.ndarray,\n    power_mW: np.ndarray,\n    title: str = \"Power\",\n    figsize: tuple = (6, 3),\n):\n    \"\"\"\n    Power vs time curve.\n\n    Args:\n        time_ms: Time points (ms).\n        power_mW: Power (mW).\n        title: Plot title.\n        figsize: Figure size.\n\n    Returns:\n        matplotlib Figure or None.\n    \"\"\"\n    if not _HAS_MPL:\n        return None\n    time_ms = np.asarray(time_ms).ravel()\n    power_mW = np.asarray(power_mW).ravel()\n    if power_mW.size != time_ms.size:\n        power_mW = np.resize(power_mW, time_ms.size)\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(time_ms, power_mW)\n    ax.set_title(title)\n    ax.set_xlabel(\"Time (ms)\")\n    ax.set_ylabel(\"Power (mW)\")\n    plt.tight_layout()\n    return fig\n"}, "src\\utils\\weight_io.py": {"content": "\"\"\"Weight save / load helpers for crossbar conductance matrices.\"\"\"\n\nfrom pathlib import Path\n\nimport numpy as np\n\n\ndef save_weights(G: np.ndarray, path: str) -> None:\n    \"\"\"\n    Save conductance matrix G to a .npy file.\n\n    Parameters\n    ----------\n    G    : Numpy array (any shape).\n    path : Destination file path (`.npy` extension added if missing).\n    \"\"\"\n    path = Path(path)\n    if path.suffix != \".npy\":\n        path = path.with_suffix(\".npy\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n    np.save(str(path), G)\n\n\ndef load_weights(path: str) -> np.ndarray:\n    \"\"\"\n    Load conductance matrix G from a .npy file.\n\n    Parameters\n    ----------\n    path : Source file path.\n\n    Returns\n    -------\n    G : Numpy float32 array.\n    \"\"\"\n    path = Path(path)\n    if path.suffix != \".npy\":\n        path = path.with_suffix(\".npy\")\n    return np.load(str(path)).astype(np.float32)\n"}, "src\\utils\\__init__.py": {"content": "\"\"\"Utilities: logger, metrics, visualization, weight I/O.\"\"\"\n\nfrom .logger import get_logger\nfrom .metrics import compute_accuracy, aggregate_energy\nfrom .visualization import plot_heatmap, plot_firing_raster, plot_power_curve\nfrom .weight_io import save_weights, load_weights\n\n__all__ = [\n    \"get_logger\",\n    \"compute_accuracy\",\n    \"aggregate_energy\",\n    \"plot_heatmap\",\n    \"plot_firing_raster\",\n    \"plot_power_curve\",\n    \"save_weights\",\n    \"load_weights\",\n]\n\n"}, "dashboard\\app.py": {"content": "\"\"\"\nNeuro-Edge Silicon Lab Console\nHardware-Accurate ReRAM SNN Simulation Dashboard\n\"\"\"\n\nimport sys\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Ensure project root is available\nsys.path.insert(0, str(Path(__file__).resolve().parent.parent))\n\nfrom src.crossbar import IdealCrossbar\nfrom src.snn import PoissonEncoder, LIFNeuron, SNNNetwork\nfrom src.hardware.energy_estimator import EnergyEstimator\nfrom src.utils.mnist_loader import get_mnist_path, load_mnist_from_path\nfrom src.utils.metrics import compute_accuracy\nfrom src.utils.weight_io import load_weights\n\n# \u2500\u2500 Page Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nst.set_page_config(\n    page_title=\"Neuro-Edge | Silicon Lab\",\n    page_icon=\"\ud83e\udde0\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# \u2500\u2500 Silicon Lab CSS Injection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nst.markdown(\"\"\"\n<style>\n/* Global Styling */\n.stApp {\n    background-color: #0B0F1A;\n}\n\n/* Glassmorphism Cards */\ndiv.stMetric, .stPlotlyChart {\n    background: #111827;\n    padding: 20px;\n    border-radius: 16px;\n    border: 1px solid rgba(0, 240, 255, 0.1);\n    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\n}\n\n/* Pulsing Header */\n@keyframes glow {\n  0% { text-shadow: 0 0 5px #00F0FF; }\n  50% { text-shadow: 0 0 15px #00F0FF; }\n  100% { text-shadow: 0 0 5px #00F0FF; }\n}\n\nh1 {\n    color: #00F0FF !important;\n    animation: glow 3s infinite;\n    letter-spacing: 1px;\n    font-weight: 700 !important;\n}\n\nh2, h3 {\n    color: #E5E7EB !important;\n    font-weight: 600 !important;\n}\n\n/* Sidebar Customization */\nsection[data-testid=\"stSidebar\"] {\n    background: #0E1424;\n    border-right: 1px solid rgba(0, 240, 255, 0.1);\n}\n\n/* Metric Tweak */\n[data-testid=\"stMetricValue\"] {\n    color: #00F0FF !important;\n}\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# \u2500\u2500 Branding \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nis_dev = \"dev\" in str(Path.cwd()) or st.sidebar.toggle(\"Force Dev Mode\", value=False)\nif is_dev:\n    st.warning(\"\ud83d\udd2c DEVELOPMENT MODE | MLC ENGINE ACTIVE\")\n\nst.markdown(\"\"\"\n<h1>Neuro-Edge Silicon Lab Console</h1>\n<p style='color:#9CA3AF; margin-top:-1rem; font-size:1.1rem;'>\nHardware-Accurate Neuromorphic Simulation Interface v1.0\n</p>\n\"\"\", unsafe_allow_html=True)\n\n# \u2500\u2500 Sidebar Control Panel \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nst.sidebar.header(\"\ud83d\udd79\ufe0f Lab Control Panel\")\nrows = st.sidebar.slider(\"Crossbar Rows\", 8, 128, 32)\ncols = st.sidebar.slider(\"Crossbar Columns\", 8, 128, 32)\ntimesteps = st.sidebar.slider(\"Integration Timesteps (ms)\", 20, 200, 50)\n\nst.sidebar.markdown(\"---\")\nst.sidebar.subheader(\"\ud83e\udde0 Neural Engine\")\n_WEIGHTS_PATH = Path(__file__).resolve().parent.parent / \"experiments\" / \"trained_weights.npy\"\n_weights_available = _WEIGHTS_PATH.exists()\n\nuse_trained = st.sidebar.checkbox(\n    \"Load Trained Weights\",\n    value=_weights_available,\n    disabled=not _weights_available\n)\n\nauto_play = st.sidebar.toggle(\"Auto-play Inference\", value=False)\nplay_speed = st.sidebar.slider(\"Cycle Rate (s)\", 0.5, 5.0, 2.0) if auto_play else 2.0\n\nst.sidebar.markdown(\"---\")\nst.sidebar.subheader(\"\ud83d\udce1 System Status\")\nst.sidebar.markdown(\"\"\"\n- \ud83d\udd0b **Power Rail**: Stable (1.2V)\n- \u2699\ufe0f **Crossbar**: Active\n- \ud83e\udde0 **SNN Engine**: Spiking\n- \ud83d\udce1 **Console**: Online\n\"\"\")\nst.sidebar.success(\"\ud83d\udfe2 Core Initialized\")\n\n# \u2500\u2500 Session State Handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nif \"mnist_data\" not in st.session_state:\n    st.session_state.mnist_data = None\nif \"current_idx\" not in st.session_state:\n    st.session_state.current_idx = 0\nif \"accuracy\" not in st.session_state:\n    st.session_state.accuracy = None\nif \"evolution_step\" not in st.session_state:\n    st.session_state.evolution_step = 0.0\n\n# \u2500\u2500 Weight Evolution Feature \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nevolution_mode = st.sidebar.toggle(\"\ud83d\udd2c Simulate Weight Evolution\", value=False)\nif evolution_mode:\n    evo_speed = st.sidebar.slider(\"Learning Rate (Sim)\", 0.01, 0.20, 0.05)\n    if st.session_state.evolution_step < 1.0:\n        st.session_state.evolution_step = min(1.0, st.session_state.evolution_step + evo_speed)\n        time.sleep(0.1)\n        st.rerun()\n    if st.sidebar.button(\"Reset Evolution\"):\n        st.session_state.evolution_step = 0.0\n        st.rerun()\n\n# \u2500\u2500 Dashboard Layout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# Row 1: Key Metrics\nm_col1, m_col2, m_col3, m_col4 = st.columns(4)\n\nwith m_col1:\n    if evolution_mode:\n        # Interpolated accuracy\n        target_acc = st.session_state.accuracy if st.session_state.accuracy else 0.82\n        current_acc = 0.10 + (target_acc - 0.10) * st.session_state.evolution_step\n        st.metric(\"In-Situ Accuracy\", f\"{current_acc:.2%}\", delta=f\"{st.session_state.evolution_step:.1%} Trained\")\n    else:\n        acc_text = f\"{st.session_state.accuracy:.2%}\" if st.session_state.accuracy else \"DORMANT\"\n        st.metric(\"Test Accuracy\", acc_text, delta=\"Optimized\" if use_trained else \"Baseline\")\nwith m_col2:\n    st.metric(\"Matrix Capacity\", f\"{rows * cols} Cells\")\nwith m_col3:\n    st.metric(\"Pulse Window\", f\"{timesteps} Timesteps\")\nwith m_col4:\n    st.metric(\"Hardware Status\", \"LEARNING...\" if (evolution_mode and st.session_state.evolution_step < 1.0) else \"STABLE\")\n\nst.divider()\n\n# Row 2: Matrix & Event Analysis\ncol_left, col_right = st.columns([1, 1.2])\n\nwith col_left:\n    st.subheader(\"\ud83d\udcca Memristive Crossbar Matrix\")\n    # Base weight generation\n    np.random.seed(42)\n    W_init = np.random.rand(rows, cols).astype(np.float32) * 0.05\n    \n    if evolution_mode and _weights_available:\n        # Load trained G (or part of it if size mismatch)\n        G_trained_full = load_weights(str(_WEIGHTS_PATH))\n        # Resize/crop to fit current UI sliders\n        G_target = np.zeros((rows, cols))\n        r_f, c_f = min(rows, G_trained_full.shape[0]), min(cols, G_trained_full.shape[1])\n        G_target[:r_f, :c_f] = G_trained_full[:r_f, :c_f]\n        \n        # Interpolate\n        alpha = st.session_state.evolution_step\n        G_display = (1 - alpha) * W_init + alpha * G_target\n    else:\n        G_display = W_init\n        \n    fig_heat = px.imshow(\n        G_display,\n        color_continuous_scale=\"Turbo\",\n        labels=dict(x=\"Column\", y=\"Row\", color=\"G (S)\"),\n        aspect=\"auto\"\n    )\n    fig_heat.update_layout(\n        margin=dict(l=0, r=0, t=10, b=0),\n        coloraxis_colorbar=dict(title=None),\n        paper_bgcolor='rgba(0,0,0,0)',\n        plot_bgcolor='rgba(0,0,0,0)',\n        font_color=\"#E5E7EB\",\n    )\n    st.plotly_chart(fig_heat, use_container_width=True)\n\nwith col_right:\n    st.subheader(\"\u26a1 Spike Event Analyzer\")\n    # Small demo run\n    n_in_s, n_out_s = min(32, rows), min(10, cols)\n    cb_s = IdealCrossbar(n_in_s, n_out_s)\n    \n    # Use current evolving weights for the demo raster too\n    W_s_init = np.maximum(np.random.randn(n_in_s, n_out_s) * 0.1, 0)\n    if evolution_mode and _weights_available:\n        G_tr_s = load_weights(str(_WEIGHTS_PATH))\n        # Match dimensions for the small subset\n        W_s_target = np.zeros((n_in_s, n_out_s))\n        r_s, c_s = min(n_in_s, G_tr_s.shape[0]), min(n_out_s, G_tr_s.shape[1])\n        W_s_target[:r_s, :c_s] = G_tr_s[:r_s, :c_s]\n        W_s = (1 - st.session_state.evolution_step) * W_s_init + st.session_state.evolution_step * W_s_target\n    else:\n        W_s = W_s_init\n        \n    cb_s.set_conductance(W_s)\n    xs = np.random.rand(n_in_s).astype(np.float32)\n    sp_out, _ = SNNNetwork(n_in_s, n_out_s, cb_s.run, encoder=PoissonEncoder(50.0), timesteps=timesteps).forward(xs, seed=42)\n    \n    t_coords, n_coords = np.where(sp_out > 0)\n    fig_raster = go.Figure()\n    fig_raster.add_trace(go.Scatter(\n        x=t_coords, y=n_coords, mode='markers',\n        marker=dict(size=6, color=\"#00F0FF\", opacity=0.8, symbol=\"square\")\n    ))\n    fig_raster.update_layout(\n        margin=dict(l=0, r=0, t=10, b=0),\n        xaxis_title=\"Time (Timesteps)\", yaxis_title=\"Neuron ID\",\n        paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n        font_color=\"#E5E7EB\",\n        xaxis=dict(gridcolor=\"rgba(255,255,255,0.05)\"),\n        yaxis=dict(gridcolor=\"rgba(255,255,255,0.05)\")\n    )\n    st.plotly_chart(fig_raster, use_container_width=True)\n\n# Row 3: Power & Inference Lab\ncol_pwr, col_inf = st.columns([1, 1.2])\n\nwith col_pwr:\n    st.subheader(\"\ud83d\udcc9 Dynamic Power Profiling\")\n    v_demo = np.random.rand(rows).astype(np.float32) * 0.5\n    energy = EnergyEstimator().energy_crossbar(v_demo, G_display)\n    t_curve = np.linspace(0, timesteps, 50)\n    p_base = energy * 1e6\n    p_curve = p_base + np.sin(t_curve * 0.5) * (p_base * 0.1)\n    \n    fig_pwr = px.line(x=t_curve, y=p_curve, labels={'x': 'Time (ms)', 'y': 'Power (uW)'})\n    fig_pwr.update_traces(line_color=\"#7C3AED\", line_width=3)\n    fig_pwr.update_layout(\n        margin=dict(l=0, r=0, t=10, b=0),\n        paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n        font_color=\"#E5E7EB\",\n        xaxis=dict(gridcolor=\"rgba(255,255,255,0.05)\"),\n        yaxis=dict(gridcolor=\"rgba(255,255,255,0.05)\")\n    )\n    st.plotly_chart(fig_pwr, use_container_width=True)\n\nwith col_inf:\n    st.subheader(\"\ud83c\udfaf Input Stimulus / Inference\")\n    if not st.session_state.mnist_data:\n        if st.button(\"\ud83d\udd25 BOOT SNN ENGINE\", type=\"primary\"):\n            path = get_mnist_path()\n            X_tr, y_tr, X_te, y_te, p_in = load_mnist_from_path(path, max_test=200)\n            st.session_state.mnist_data = (X_te, y_te, p_in)\n            st.rerun()\n    else:\n        X_test, y_test, p_pixels = st.session_state.mnist_data\n        \n        # In evolution mode, we use the evolving weights even for the main inference showcase\n        if evolution_mode and _weights_available:\n            alpha = st.session_state.evolution_step\n            G_full_trained = load_weights(str(_WEIGHTS_PATH))\n            W_m_init = np.maximum(np.random.randn(p_pixels, 10) * 0.01, 0)\n            W_m = (1 - alpha) * W_m_init + alpha * G_full_trained\n        elif use_trained and _weights_available:\n            W_m = load_weights(str(_WEIGHTS_PATH))\n            if W_m.shape != (p_pixels, 10):\n                W_m = np.maximum(np.random.randn(p_pixels, 10) * 0.01, 0)\n        else:\n            W_m = np.maximum(np.random.randn(p_pixels, 10) * 0.01, 0)\n\n        cb_m = IdealCrossbar(p_pixels, 10)\n        cb_m.set_conductance(W_m)\n        snn_m = SNNNetwork(p_pixels, 10, cb_m.run, encoder=PoissonEncoder(100.0), timesteps=timesteps)\n\n        # Session Accuracy Cache\n        if st.session_state.accuracy is None:\n            with st.spinner(\"Analyzing Benchmarks...\"):\n                logits = np.zeros((len(X_test), 10))\n                for i in range(len(X_test)):\n                    _, tot = snn_m.forward(X_test[i])\n                    logits[i] = tot\n                st.session_state.accuracy = compute_accuracy(logits, y_test)\n                st.rerun()\n\n        # Inference Display\n        idx = st.session_state.current_idx\n        _, t_spikes = snn_m.forward(X_test[idx])\n        pred = int(np.argmax(t_spikes))\n        truth = int(y_test[idx])\n        \n        inf_c1, inf_c2 = st.columns([1, 2])\n        with inf_c1:\n            img = (X_test[idx].reshape(28, 28) * 255).astype(np.uint8)\n            st.image(img, caption=f\"Sample ID: #{idx}\", use_container_width=True)\n            \n            # Result Badge\n            status_color = \"#00FF9C\" if pred == truth else \"#FFB020\"\n            st.markdown(f\"\"\"\n            <div style='background:rgba(0,0,0,0.3); padding:10px; border-radius:10px; border-left:4px solid {status_color}'>\n                <span style='color:{status_color}; font-size:1.2rem; font-weight:bold;'>DECISION: {pred}</span><br/>\n                <span style='color:#9CA3AF;'>EXPECTED: {truth}</span>\n            </div>\n            \"\"\", unsafe_allow_html=True)\n        \n        with inf_c2:\n            labels = [str(i) for i in range(10)]\n            fig_bar = px.bar(x=labels, y=t_spikes, labels={'x': 'Class', 'y': 'Firing Rate'})\n            fig_bar.update_traces(marker_color=\"#00F0FF\")\n            fig_bar.update_layout(\n                height=220, margin=dict(l=0, r=0, t=10, b=0),\n                paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n                font_color=\"#E5E7EB\",\n                xaxis=dict(gridcolor=\"rgba(255,255,255,0.05)\"),\n                yaxis=dict(gridcolor=\"rgba(255,255,255,0.05)\")\n            )\n            st.plotly_chart(fig_bar, use_container_width=True)\n\n        if auto_play:\n            time.sleep(play_speed)\n            st.session_state.current_idx = (idx + 1) % len(X_test)\n            st.rerun()\n\n\nst.divider()\nst.caption(\"\u00a9 2026 Neuro-Edge ReRAM Research Platform | Silicon Lab Console\")\n\n\n\n\n"}, "configs\\ideal.yaml": {"content": "# Ideal crossbar: no noise, no IR drop, no variability\ncrossbar:\n  rows: 64\n  cols: 64\n  precision: float32\nseed: 42\n"}, "configs\\non_ideal.yaml": {"content": "# Non-ideal crossbar: noise, IR drop, variability\ncrossbar:\n  rows: 64\n  cols: 64\n  precision: float32\nnoise:\n  enabled: true\n  std: 0.01\nir_drop:\n  enabled: true\n  row_resistance: 0.1\n  col_resistance: 0.1\nvariability:\n  enabled: true\n  conductance_std: 0.02\nseed: 42\n"}, "configs\\snn_config.yaml": {"content": "# SNN: spike encoding, neuron params, timesteps\nencoding:\n  type: poisson\n  max_rate: 100.0\nneuron:\n  threshold: 1.0\n  leak: 0.99\n  reset: 0.0\ntimesteps: 100\nseed: 42\n"}, "verilog/experimental\\crossbar_mlc_controller.sv": {"content": "/**\n * crossbar_mlc_controller.sv\n *\n * EXPERIMENTAL Development Module\n *\n * Implementation of a Multi-Level Cell (MLC) ReRAM controller.\n * Unlike SLC (Single-Level Cell) which stores 1 bit, this module\n * simulates 4-bit per cell (16 conductance levels) in hardware logic.\n */\n\nmodule crossbar_mlc_controller #(\n    parameter int ROWS = 32,\n    parameter int COLS = 10,\n    parameter int WEIGHT_PRECISION = 4 // 4-bit MLC\n)(\n    input  logic              clk,\n    input  logic              rst_n,\n    \n    // Virtual Crossbar Interface\n    input  logic [ROWS-1:0]   row_spikes,\n    output logic [31:0]       weighted_sum[COLS],\n    \n    // Weight Programming (MLC)\n    input  logic              prog_en,\n    input  logic [4:0]        addr_row,\n    input  logic [3:0]        addr_col,\n    input  logic [WEIGHT_PRECISION-1:0] weight_val\n);\n\n    // MLC Weight Matrix (Internal emulation)\n    logic [WEIGHT_PRECISION-1:0] G_matrix[ROWS][COLS];\n\n    // Inference Logic\n    always_ff @(posedge clk or negedge rst_n) begin\n        if (!rst_n) begin\n            for (int i=0; i<ROWS; i++) begin\n                for (int j=0; j<COLS; j++) begin\n                    G_matrix[i][j] <= 0;\n                end\n            end\n        end else if (prog_en) begin\n            G_matrix[addr_row][addr_col] <= weight_val;\n        end\n    end\n\n    // Combinatorial VMM Integration\n    always_comb begin\n        for (int j=0; j<COLS; j++) begin\n            weighted_sum[j] = 0;\n            for (int i=0; i<ROWS; i++) begin\n                if (row_spikes[i]) begin\n                    weighted_sum[j] += G_matrix[i][j];\n                end\n            end\n        end\n    end\n\nendmodule\n"}}